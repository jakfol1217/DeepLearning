{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25816b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff4b32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kacper\n"
     ]
    }
   ],
   "source": [
    "%cd\n",
    "from DeepLearning.Project2.data_loading_preparation import load_audio_dataloaders_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e9346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generic_classifier(input_size, hidden_size, output_size):\n",
    "    classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "    )\n",
    "    return classifier\n",
    "\n",
    "class RecurrentSimple(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size, \n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        num_classes = 12,\n",
    "        avgpool_dim = 32,\n",
    "        add_dropout=True,\n",
    "        classifier_size = 512\n",
    "    ):       \n",
    "        super().__init__()\n",
    "        self.normalization = nn.BatchNorm1d(input_size)\n",
    "        self.features = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(avgpool_dim)\n",
    "        self.classifier = get_generic_classifier(\n",
    "            hidden_size * avgpool_dim,\n",
    "            classifier_size,\n",
    "            num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.normalization(x)\n",
    "        x = x.mT\n",
    "        x, _ = self.features(x)\n",
    "        x = x.mT\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class LSTMSimple(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size, \n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        num_classes = 12,\n",
    "        avgpool_dim = 32,\n",
    "        classifier_size = 512,\n",
    "        add_dropout=True\n",
    "    ):       \n",
    "        super().__init__()\n",
    "        self.normalization = nn.BatchNorm1d(input_size)\n",
    "        self.features = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(avgpool_dim)\n",
    "        self.classifier = get_generic_classifier(\n",
    "            hidden_size * avgpool_dim,\n",
    "            classifier_size,\n",
    "            num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.normalization(x)\n",
    "        x = x.mT\n",
    "        x, _ = self.features(x)\n",
    "        x = x.mT\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c4a3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(model, dataloader, reccurrent, training_device='cuda'):\n",
    "    with torch.no_grad():\n",
    "        model.to(training_device)\n",
    "        correct = 0\n",
    "        all_so_far = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.float().to(training_device), labels.float().to(training_device)\n",
    "            yhat = model(inputs)\n",
    "            pred = torch.argmax(yhat, dim=1)\n",
    "\n",
    "            \n",
    "            all_so_far += labels.size().numel()\n",
    "            correct += torch.sum(pred.eq(labels)).item()\n",
    "    return correct/all_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458e100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_to_ram(model):\n",
    "    from copy import deepcopy\n",
    "    return deepcopy(model).cpu()\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience = 3, backup_method=backup_to_ram):\n",
    "        self.patience = patience\n",
    "        self.current = 0\n",
    "        \n",
    "        self.backup_method = backup_method\n",
    "        \n",
    "        self.best_backup = None\n",
    "        self.best_accuracy = 0.\n",
    "\n",
    "    def should_continue(self, accuracy, model = None):\n",
    "        if self.best_accuracy < accuracy:\n",
    "            self.current = 0\n",
    "            self.best_accuracy = accuracy\n",
    "            if model is not None:\n",
    "                self.best_backup = self.backup_method(model)\n",
    "            return True\n",
    "        \n",
    "        self.current += 1\n",
    "        \n",
    "        if self.current >= self.patience:\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b5429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(experiment_name, train_func, run, train_params=None):\n",
    "    path = f\"experiments_rnn/{experiment_name}_run_{run}_\"\n",
    "    print(\"Running experiment for \", path[:-1])\n",
    "    \n",
    "    import os\n",
    "    try:\n",
    "        if os.stat(path + \"report.json\").st_size != 0:\n",
    "            print(\"Report exists already for \" + path[:-1] + \". Skipping...\")\n",
    "            return\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    model, trajectory, validation_accuracy = train_func(train_params)\n",
    "    \n",
    "    with open(path + \"report.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"name\": experiment_name,\n",
    "                \"train_params\": train_params,\n",
    "                \"run\": run,\n",
    "                \"best_accuracy_validation\": validation_accuracy,\n",
    "                \"time_generated\": datetime.datetime.now().isoformat(),\n",
    "                \"trajectory\": trajectory\n",
    "            },\n",
    "            f\n",
    "        )\n",
    "    torch.save(model, path + \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c815aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_device = \"cuda\"\n",
    "device = \"cuda\"\n",
    "max_epochs = 250\n",
    "\n",
    "def oneRNN(train_params):  \n",
    "    train, test, val = load_audio_dataloaders_validation(bs=128)\n",
    "    criterion_weights = torch.tensor([1.] * 11 + [0.1]).to(device)\n",
    "    model = RecurrentSimple(**train_params).to(training_device)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=criterion_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=[0.9, 0.999], eps=10e-8)\n",
    "    early_stopper = EarlyStopper(patience = 10)\n",
    "    trajectory = []\n",
    "    model.to(training_device)\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        for x, y in train:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.float().to(device), y.float().to(device)\n",
    "            (yhat) = model(x)\n",
    "            loss = criterion(yhat.softmax(1), y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        validation_accuracy = eval_accuracy(model, val, training_device)\n",
    "        \n",
    "        print(\"Epoch: {}, Accuracy on validation set: {}\".format(epoch, validation_accuracy))\n",
    "        \n",
    "        trajectory.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"validation\": validation_accuracy,\n",
    "        })\n",
    "        \n",
    "        if not early_stopper.should_continue(validation_accuracy, model):\n",
    "            print(\"Early stop\")\n",
    "            model = early_stopper.best_backup\n",
    "            model = model.to(device)\n",
    "            break\n",
    "\n",
    "    return model, trajectory, validation_accuracy\n",
    "\n",
    "def oneLSTM(train_params):  \n",
    "    train, test, val = load_audio_dataloaders_validation(bs=128)\n",
    "    criterion_weights = torch.tensor([1.] * 11 + [0.1]).to(device)\n",
    "    model = LSTMSimple(**train_params).to(training_device)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=criterion_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=[0.9, 0.999], eps=10e-8)\n",
    "    early_stopper = EarlyStopper(patience = 10)\n",
    "    trajectory = []\n",
    "    model.to(training_device)\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        for x, y in train:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.float().to(device), y.float().to(device)\n",
    "            (yhat) = model(x)\n",
    "            loss = criterion(yhat.softmax(1), y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        validation_accuracy = eval_accuracy(model, val, training_device)\n",
    "        \n",
    "        print(\"Epoch: {}, Accuracy on validation set: {}\".format(epoch, validation_accuracy))\n",
    "        \n",
    "        trajectory.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"validation\": validation_accuracy,\n",
    "        })\n",
    "        \n",
    "        if not early_stopper.should_continue(validation_accuracy, model):\n",
    "            print(\"Early stop\")\n",
    "            model = early_stopper.best_backup\n",
    "            model = model.to(device)\n",
    "            break\n",
    "\n",
    "    return model, trajectory, validation_accuracy\n",
    "\n",
    "def equalRNN(train_params):  \n",
    "    train, test, val = load_audio_dataloaders_validation(bs=128)\n",
    "    model = RecurrentSimple(**train_params).to(training_device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=[0.9, 0.999], eps=10e-8)\n",
    "    early_stopper = EarlyStopper(patience = 10)\n",
    "    trajectory = []\n",
    "    model.to(training_device)\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        for x, y in train:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.float().to(device), y.float().to(device)\n",
    "            (yhat) = model(x)\n",
    "            loss = criterion(yhat.softmax(1), y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        validation_accuracy = eval_accuracy(model, val, training_device)\n",
    "        \n",
    "        print(\"Epoch: {}, Accuracy on validation set: {}\".format(epoch, validation_accuracy))\n",
    "        \n",
    "        trajectory.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"validation\": validation_accuracy,\n",
    "        })\n",
    "        \n",
    "        if not early_stopper.should_continue(validation_accuracy, model):\n",
    "            print(\"Early stop\")\n",
    "            model = early_stopper.best_backup\n",
    "            model = model.to(device)\n",
    "            break\n",
    "\n",
    "    return model, trajectory, validation_accuracy\n",
    "\n",
    "def equalLSTM(train_params):  \n",
    "    train, test, val = load_audio_dataloaders_validation(bs=128)\n",
    "    model = LSTMSimple(**train_params).to(training_device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=[0.9, 0.999], eps=10e-8)\n",
    "    early_stopper = EarlyStopper(patience = 10)\n",
    "    trajectory = []\n",
    "    model.to(training_device)\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        for x, y in train:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.float().to(device), y.float().to(device)\n",
    "            (yhat) = model(x)\n",
    "            loss = criterion(yhat.softmax(1), y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        validation_accuracy = eval_accuracy(model, val, training_device)\n",
    "        \n",
    "        print(\"Epoch: {}, Accuracy on validation set: {}\".format(epoch, validation_accuracy))\n",
    "        \n",
    "        trajectory.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"validation\": validation_accuracy,\n",
    "        })\n",
    "        \n",
    "        if not early_stopper.should_continue(validation_accuracy, model):\n",
    "            print(\"Early stop\")\n",
    "            model = early_stopper.best_backup\n",
    "            model = model.to(device)\n",
    "            break\n",
    "\n",
    "    return model, trajectory, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24c033f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"tiny\": {\n",
    "        \"hidden_size\": 40,\n",
    "        \"input_size\": 20,\n",
    "        \"num_layers\": 1,\n",
    "    },\n",
    "    \"tinydouble\": {\n",
    "        \"hidden_size\": 40,\n",
    "        \"input_size\": 20,\n",
    "        \"num_layers\": 2,\n",
    "    },\n",
    "    \"tinytriple\": {\n",
    "        \"hidden_size\": 40,\n",
    "        \"input_size\": 20,\n",
    "        \"num_layers\": 3,\n",
    "    },\n",
    "    \"wide\": {\n",
    "        \"hidden_size\": 80,\n",
    "        \"input_size\": 20,\n",
    "        \"num_layers\": 1,\n",
    "    },\n",
    "    \"widedouble\": {\n",
    "        \"hidden_size\": 80,\n",
    "        \"input_size\": 20,\n",
    "        \"num_layers\": 2,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "experiment_list = [\n",
    "    (\n",
    "        f\"trainer_{trainer.__name__}_params_{param_name}\", \n",
    "        trainer,\n",
    "        str(run),\n",
    "        param\n",
    "    )\n",
    "    for run in range(1, 11) \n",
    "    for trainer in [oneRNN, oneLSTM]\n",
    "    for param_name, param in param_grid.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60bed5d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2023-04-16T10:48:11.733273 Experiment: trainer_oneRNN_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tiny_run_1\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tiny_run_1. Skipping...\n",
      "Time: 2023-04-16T10:48:11.734560 Experiment: trainer_oneRNN_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinydouble_run_1\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinydouble_run_1. Skipping...\n",
      "Time: 2023-04-16T10:48:11.734592 Experiment: trainer_oneRNN_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinytriple_run_1\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinytriple_run_1. Skipping...\n",
      "Time: 2023-04-16T10:48:11.734626 Experiment: trainer_oneRNN_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_wide_run_1\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_wide_run_1. Skipping...\n",
      "Time: 2023-04-16T10:48:11.734650 Experiment: trainer_oneRNN_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_widedouble_run_1\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_widedouble_run_1. Skipping...\n",
      "Time: 2023-04-16T10:48:11.734898 Experiment: trainer_oneLSTM_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tiny_run_1\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tiny_run_1. Skipping...\n",
      "Time: 2023-04-16T10:48:11.735356 Experiment: trainer_oneLSTM_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinydouble_run_1\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinydouble_run_1. Skipping...\n",
      "Time: 2023-04-16T10:48:11.735397 Experiment: trainer_oneLSTM_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinytriple_run_1\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinytriple_run_1. Skipping...\n",
      "Time: 2023-04-16T10:48:11.735564 Experiment: trainer_oneLSTM_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_wide_run_1\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_wide_run_1. Skipping...\n",
      "Time: 2023-04-16T10:48:11.735588 Experiment: trainer_oneLSTM_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_widedouble_run_1\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_widedouble_run_1. Skipping...\n",
      "Time: 2023-04-16T10:48:11.735610 Experiment: trainer_oneRNN_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tiny_run_2\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tiny_run_2. Skipping...\n",
      "Time: 2023-04-16T10:48:11.735630 Experiment: trainer_oneRNN_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinydouble_run_2\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinydouble_run_2. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736051 Experiment: trainer_oneRNN_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinytriple_run_2\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinytriple_run_2. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736078 Experiment: trainer_oneRNN_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_wide_run_2\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_wide_run_2. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736104 Experiment: trainer_oneRNN_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_widedouble_run_2\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_widedouble_run_2. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736128 Experiment: trainer_oneLSTM_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tiny_run_2\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tiny_run_2. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736148 Experiment: trainer_oneLSTM_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinydouble_run_2\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinydouble_run_2. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736170 Experiment: trainer_oneLSTM_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinytriple_run_2\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinytriple_run_2. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736209 Experiment: trainer_oneLSTM_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_wide_run_2\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_wide_run_2. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736230 Experiment: trainer_oneLSTM_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_widedouble_run_2\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_widedouble_run_2. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736265 Experiment: trainer_oneRNN_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tiny_run_3\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tiny_run_3. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736286 Experiment: trainer_oneRNN_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinydouble_run_3\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinydouble_run_3. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736316 Experiment: trainer_oneRNN_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinytriple_run_3\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinytriple_run_3. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736344 Experiment: trainer_oneRNN_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_wide_run_3\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_wide_run_3. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736363 Experiment: trainer_oneRNN_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_widedouble_run_3\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_widedouble_run_3. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736398 Experiment: trainer_oneLSTM_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tiny_run_3\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tiny_run_3. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736418 Experiment: trainer_oneLSTM_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinydouble_run_3\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinydouble_run_3. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736450 Experiment: trainer_oneLSTM_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinytriple_run_3\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinytriple_run_3. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736493 Experiment: trainer_oneLSTM_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_wide_run_3\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_wide_run_3. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736532 Experiment: trainer_oneLSTM_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_widedouble_run_3\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_widedouble_run_3. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736559 Experiment: trainer_oneRNN_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tiny_run_4\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tiny_run_4. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736609 Experiment: trainer_oneRNN_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinydouble_run_4\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinydouble_run_4. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736647 Experiment: trainer_oneRNN_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinytriple_run_4\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinytriple_run_4. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736671 Experiment: trainer_oneRNN_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_wide_run_4\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_wide_run_4. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736710 Experiment: trainer_oneRNN_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_widedouble_run_4\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_widedouble_run_4. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736733 Experiment: trainer_oneLSTM_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tiny_run_4\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tiny_run_4. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736751 Experiment: trainer_oneLSTM_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinydouble_run_4\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinydouble_run_4. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736768 Experiment: trainer_oneLSTM_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinytriple_run_4\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinytriple_run_4. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736786 Experiment: trainer_oneLSTM_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_wide_run_4\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_wide_run_4. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736804 Experiment: trainer_oneLSTM_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_widedouble_run_4\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_widedouble_run_4. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736821 Experiment: trainer_oneRNN_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tiny_run_5\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tiny_run_5. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736839 Experiment: trainer_oneRNN_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinydouble_run_5\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinydouble_run_5. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736857 Experiment: trainer_oneRNN_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinytriple_run_5\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinytriple_run_5. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736874 Experiment: trainer_oneRNN_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_wide_run_5\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_wide_run_5. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736892 Experiment: trainer_oneRNN_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_widedouble_run_5\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_widedouble_run_5. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736910 Experiment: trainer_oneLSTM_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tiny_run_5\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tiny_run_5. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736943 Experiment: trainer_oneLSTM_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinydouble_run_5\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinydouble_run_5. Skipping...\n",
      "Time: 2023-04-16T10:48:11.736963 Experiment: trainer_oneLSTM_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinytriple_run_5\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinytriple_run_5. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737313 Experiment: trainer_oneLSTM_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_wide_run_5\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_wide_run_5. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737354 Experiment: trainer_oneLSTM_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_widedouble_run_5\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_widedouble_run_5. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737384 Experiment: trainer_oneRNN_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tiny_run_6\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tiny_run_6. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737412 Experiment: trainer_oneRNN_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinydouble_run_6\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinydouble_run_6. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737461 Experiment: trainer_oneRNN_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinytriple_run_6\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinytriple_run_6. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737489 Experiment: trainer_oneRNN_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_wide_run_6\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_wide_run_6. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737512 Experiment: trainer_oneRNN_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_widedouble_run_6\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_widedouble_run_6. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737531 Experiment: trainer_oneLSTM_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tiny_run_6\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tiny_run_6. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737565 Experiment: trainer_oneLSTM_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinydouble_run_6\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinydouble_run_6. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737591 Experiment: trainer_oneLSTM_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinytriple_run_6\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinytriple_run_6. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737610 Experiment: trainer_oneLSTM_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_wide_run_6\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_wide_run_6. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737647 Experiment: trainer_oneLSTM_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_widedouble_run_6\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_widedouble_run_6. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737667 Experiment: trainer_oneRNN_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tiny_run_7\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tiny_run_7. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737684 Experiment: trainer_oneRNN_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinydouble_run_7\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinydouble_run_7. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737708 Experiment: trainer_oneRNN_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinytriple_run_7\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinytriple_run_7. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737765 Experiment: trainer_oneRNN_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_wide_run_7\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_wide_run_7. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737785 Experiment: trainer_oneRNN_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_widedouble_run_7\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_widedouble_run_7. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737809 Experiment: trainer_oneLSTM_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tiny_run_7\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tiny_run_7. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737832 Experiment: trainer_oneLSTM_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinydouble_run_7\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinydouble_run_7. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737850 Experiment: trainer_oneLSTM_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinytriple_run_7\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinytriple_run_7. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737877 Experiment: trainer_oneLSTM_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_wide_run_7\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_wide_run_7. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737911 Experiment: trainer_oneLSTM_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_widedouble_run_7\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_widedouble_run_7. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737930 Experiment: trainer_oneRNN_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tiny_run_8\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tiny_run_8. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737948 Experiment: trainer_oneRNN_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinydouble_run_8\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinydouble_run_8. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737965 Experiment: trainer_oneRNN_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinytriple_run_8\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinytriple_run_8. Skipping...\n",
      "Time: 2023-04-16T10:48:11.737985 Experiment: trainer_oneRNN_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_wide_run_8\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_wide_run_8. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738460 Experiment: trainer_oneRNN_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_widedouble_run_8\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_widedouble_run_8. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738491 Experiment: trainer_oneLSTM_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tiny_run_8\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tiny_run_8. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738510 Experiment: trainer_oneLSTM_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinydouble_run_8\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinydouble_run_8. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738527 Experiment: trainer_oneLSTM_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinytriple_run_8\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinytriple_run_8. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738543 Experiment: trainer_oneLSTM_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_wide_run_8\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_wide_run_8. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738565 Experiment: trainer_oneLSTM_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_widedouble_run_8\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_widedouble_run_8. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738582 Experiment: trainer_oneRNN_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tiny_run_9\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tiny_run_9. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738598 Experiment: trainer_oneRNN_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinydouble_run_9\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinydouble_run_9. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738614 Experiment: trainer_oneRNN_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinytriple_run_9\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinytriple_run_9. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738635 Experiment: trainer_oneRNN_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_wide_run_9\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_wide_run_9. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738658 Experiment: trainer_oneRNN_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_widedouble_run_9\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_widedouble_run_9. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738676 Experiment: trainer_oneLSTM_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tiny_run_9\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tiny_run_9. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738694 Experiment: trainer_oneLSTM_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinydouble_run_9\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinydouble_run_9. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738714 Experiment: trainer_oneLSTM_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinytriple_run_9\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_tinytriple_run_9. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738730 Experiment: trainer_oneLSTM_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_wide_run_9\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_wide_run_9. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738747 Experiment: trainer_oneLSTM_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_widedouble_run_9\n",
      "Report exists already for experiments_rnn/trainer_oneLSTM_params_widedouble_run_9. Skipping...\n",
      "Time: 2023-04-16T10:48:11.738770 Experiment: trainer_oneRNN_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tiny_run_10\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tiny_run_10. Skipping...\n",
      "Time: 2023-04-16T10:48:11.739235 Experiment: trainer_oneRNN_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinydouble_run_10\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinydouble_run_10. Skipping...\n",
      "Time: 2023-04-16T10:48:11.739280 Experiment: trainer_oneRNN_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_tinytriple_run_10\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_tinytriple_run_10. Skipping...\n",
      "Time: 2023-04-16T10:48:11.739314 Experiment: trainer_oneRNN_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_wide_run_10\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_wide_run_10. Skipping...\n",
      "Time: 2023-04-16T10:48:11.739342 Experiment: trainer_oneRNN_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneRNN_params_widedouble_run_10\n",
      "Report exists already for experiments_rnn/trainer_oneRNN_params_widedouble_run_10. Skipping...\n",
      "Time: 2023-04-16T10:48:11.739359 Experiment: trainer_oneLSTM_params_tiny\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tiny_run_10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Accuracy on validation set: 0.19982902329557597\n",
      "Epoch: 2, Accuracy on validation set: 0.23338320153879033\n",
      "Epoch: 3, Accuracy on validation set: 0.2949348151314383\n",
      "Epoch: 4, Accuracy on validation set: 0.3714468903611883\n",
      "Epoch: 5, Accuracy on validation set: 0.3584099166488566\n",
      "Epoch: 6, Accuracy on validation set: 0.4238085060910451\n",
      "Epoch: 7, Accuracy on validation set: 0.4045736268433426\n",
      "Epoch: 8, Accuracy on validation set: 0.46056849754220985\n",
      "Epoch: 9, Accuracy on validation set: 0.4556529172900192\n",
      "Epoch: 10, Accuracy on validation set: 0.43876896772814705\n",
      "Epoch: 11, Accuracy on validation set: 0.4761701218209019\n",
      "Epoch: 12, Accuracy on validation set: 0.5069459286172259\n",
      "Epoch: 13, Accuracy on validation set: 0.4355631545201966\n",
      "Epoch: 14, Accuracy on validation set: 0.5024577901260954\n",
      "Epoch: 15, Accuracy on validation set: 0.4900619790553537\n",
      "Epoch: 16, Accuracy on validation set: 0.5099380209446462\n",
      "Epoch: 17, Accuracy on validation set: 0.4939089549048942\n",
      "Epoch: 18, Accuracy on validation set: 0.5071596494977559\n",
      "Epoch: 19, Accuracy on validation set: 0.5360119683693096\n",
      "Epoch: 20, Accuracy on validation set: 0.5043812780508655\n",
      "Epoch: 21, Accuracy on validation set: 0.5351570848471896\n",
      "Epoch: 22, Accuracy on validation set: 0.5387903398162001\n",
      "Epoch: 23, Accuracy on validation set: 0.5503312673648215\n",
      "Epoch: 24, Accuracy on validation set: 0.5680701004488139\n",
      "Epoch: 25, Accuracy on validation set: 0.4904894208164138\n",
      "Epoch: 26, Accuracy on validation set: 0.516135926480017\n",
      "Epoch: 27, Accuracy on validation set: 0.5437059200683907\n",
      "Epoch: 28, Accuracy on validation set: 0.5494763838427015\n",
      "Epoch: 29, Accuracy on validation set: 0.5291729001923487\n",
      "Epoch: 30, Accuracy on validation set: 0.5223338320153879\n",
      "Epoch: 31, Accuracy on validation set: 0.553964522333832\n",
      "Epoch: 32, Accuracy on validation set: 0.5180594144047873\n",
      "Epoch: 33, Accuracy on validation set: 0.5345159222055995\n",
      "Epoch: 34, Accuracy on validation set: 0.5246847617012182\n",
      "Early stop\n",
      "Time: 2023-04-16T10:52:15.658901 Experiment: trainer_oneLSTM_params_tinydouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinydouble_run_10\n",
      "Epoch: 1, Accuracy on validation set: 0.16801361122926414\n",
      "Epoch: 2, Accuracy on validation set: 0.2177796682262867\n",
      "Epoch: 3, Accuracy on validation set: 0.24053594215227564\n",
      "Epoch: 4, Accuracy on validation set: 0.2575499787324543\n",
      "Epoch: 5, Accuracy on validation set: 0.33071033602722244\n",
      "Epoch: 6, Accuracy on validation set: 0.3600595491280306\n",
      "Epoch: 7, Accuracy on validation set: 0.3849425776265419\n",
      "Epoch: 8, Accuracy on validation set: 0.427903019991493\n",
      "Epoch: 9, Accuracy on validation set: 0.43811144193960017\n",
      "Epoch: 10, Accuracy on validation set: 0.44130157379838364\n",
      "Epoch: 11, Accuracy on validation set: 0.44895789025946403\n",
      "Epoch: 12, Accuracy on validation set: 0.45555082943428327\n",
      "Epoch: 13, Accuracy on validation set: 0.5350914504466184\n",
      "Epoch: 14, Accuracy on validation set: 0.5233943002977456\n",
      "Epoch: 15, Accuracy on validation set: 0.4478945129732029\n",
      "Epoch: 16, Accuracy on validation set: 0.4857507443641004\n",
      "Epoch: 17, Accuracy on validation set: 0.4987239472564866\n",
      "Epoch: 18, Accuracy on validation set: 0.512760527435134\n",
      "Epoch: 19, Accuracy on validation set: 0.48362398979157806\n",
      "Epoch: 20, Accuracy on validation set: 0.4857507443641004\n",
      "Epoch: 21, Accuracy on validation set: 0.5544449170565716\n",
      "Epoch: 22, Accuracy on validation set: 0.5274351339855381\n",
      "Epoch: 23, Accuracy on validation set: 0.47660569970225436\n",
      "Epoch: 24, Accuracy on validation set: 0.5404083368779243\n",
      "Epoch: 25, Accuracy on validation set: 0.5329646958740961\n",
      "Epoch: 26, Accuracy on validation set: 0.4795831561037856\n",
      "Epoch: 27, Accuracy on validation set: 0.5097830710336028\n",
      "Epoch: 28, Accuracy on validation set: 0.4810718843045513\n",
      "Epoch: 29, Accuracy on validation set: 0.5586984262016164\n",
      "Epoch: 30, Accuracy on validation set: 0.5746490854955338\n",
      "Epoch: 31, Accuracy on validation set: 0.5021267545725223\n",
      "Epoch: 32, Accuracy on validation set: 0.5021267545725223\n",
      "Epoch: 33, Accuracy on validation set: 0.5448745214802212\n",
      "Epoch: 34, Accuracy on validation set: 0.533815397703105\n",
      "Epoch: 35, Accuracy on validation set: 0.5246703530412591\n",
      "Epoch: 36, Accuracy on validation set: 0.5580603998298597\n",
      "Epoch: 37, Accuracy on validation set: 0.5353041259038707\n",
      "Epoch: 38, Accuracy on validation set: 0.5718843045512548\n",
      "Epoch: 39, Accuracy on validation set: 0.45831561037856233\n",
      "Epoch: 40, Accuracy on validation set: 0.5606125053168864\n",
      "Early stop\n",
      "Time: 2023-04-16T10:57:07.597705 Experiment: trainer_oneLSTM_params_tinytriple\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_tinytriple_run_10\n",
      "Epoch: 1, Accuracy on validation set: 0.1631567751542225\n",
      "Epoch: 2, Accuracy on validation set: 0.19910657306955967\n",
      "Epoch: 3, Accuracy on validation set: 0.23909806424165073\n",
      "Epoch: 4, Accuracy on validation set: 0.2542012337800468\n",
      "Epoch: 5, Accuracy on validation set: 0.2752605828547118\n",
      "Epoch: 6, Accuracy on validation set: 0.29589449053392897\n",
      "Epoch: 7, Accuracy on validation set: 0.31440119123590726\n",
      "Epoch: 8, Accuracy on validation set: 0.32227185705169115\n",
      "Epoch: 9, Accuracy on validation set: 0.34950010636034884\n",
      "Epoch: 10, Accuracy on validation set: 0.3333333333333333\n",
      "Epoch: 11, Accuracy on validation set: 0.3112103807700489\n",
      "Epoch: 12, Accuracy on validation set: 0.37566475218038714\n",
      "Epoch: 13, Accuracy on validation set: 0.34992554775579665\n",
      "Epoch: 14, Accuracy on validation set: 0.36417783450329716\n",
      "Epoch: 15, Accuracy on validation set: 0.38970431823016377\n",
      "Epoch: 16, Accuracy on validation set: 0.3639651138055733\n",
      "Epoch: 17, Accuracy on validation set: 0.39629865985960433\n",
      "Epoch: 18, Accuracy on validation set: 0.34630929589449055\n",
      "Epoch: 19, Accuracy on validation set: 0.34354392682407997\n",
      "Epoch: 20, Accuracy on validation set: 0.414167198468411\n",
      "Epoch: 21, Accuracy on validation set: 0.39544777706870876\n",
      "Epoch: 22, Accuracy on validation set: 0.41118910870027653\n",
      "Epoch: 23, Accuracy on validation set: 0.40395660497766434\n",
      "Epoch: 24, Accuracy on validation set: 0.45522229312912144\n",
      "Epoch: 25, Accuracy on validation set: 0.43097213358859815\n",
      "Epoch: 26, Accuracy on validation set: 0.41735800893426933\n",
      "Epoch: 27, Accuracy on validation set: 0.41267815358434373\n",
      "Epoch: 28, Accuracy on validation set: 0.389278876834716\n",
      "Epoch: 29, Accuracy on validation set: 0.4162944054456499\n",
      "Epoch: 30, Accuracy on validation set: 0.44011912359072536\n",
      "Epoch: 31, Accuracy on validation set: 0.4596894277813231\n",
      "Epoch: 32, Accuracy on validation set: 0.4930865773239736\n",
      "Epoch: 33, Accuracy on validation set: 0.49266113592852584\n",
      "Epoch: 34, Accuracy on validation set: 0.4779834077855775\n",
      "Epoch: 35, Accuracy on validation set: 0.4107636673048288\n",
      "Epoch: 36, Accuracy on validation set: 0.41161455009572434\n",
      "Epoch: 37, Accuracy on validation set: 0.44990427568602426\n",
      "Epoch: 38, Accuracy on validation set: 0.44628802382471816\n",
      "Epoch: 39, Accuracy on validation set: 0.44862795149968093\n",
      "Epoch: 40, Accuracy on validation set: 0.4411827270793448\n",
      "Epoch: 41, Accuracy on validation set: 0.398851308232291\n",
      "Epoch: 42, Accuracy on validation set: 0.44288449266113594\n",
      "Early stop\n",
      "Time: 2023-04-16T11:02:23.224840 Experiment: trainer_oneLSTM_params_wide\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_wide_run_10\n",
      "Epoch: 1, Accuracy on validation set: 0.1996612322676265\n",
      "Epoch: 2, Accuracy on validation set: 0.24221892864704636\n",
      "Epoch: 3, Accuracy on validation set: 0.27757781071352955\n",
      "Epoch: 4, Accuracy on validation set: 0.28287105653186534\n",
      "Epoch: 5, Accuracy on validation set: 0.31780647893288166\n",
      "Epoch: 6, Accuracy on validation set: 0.3353800550497565\n",
      "Epoch: 7, Accuracy on validation set: 0.3525301715011645\n",
      "Epoch: 8, Accuracy on validation set: 0.3897946220622486\n",
      "Epoch: 9, Accuracy on validation set: 0.380690239254711\n",
      "Epoch: 10, Accuracy on validation set: 0.3866186745712471\n",
      "Epoch: 11, Accuracy on validation set: 0.39021808172771544\n",
      "Epoch: 12, Accuracy on validation set: 0.3859834850730468\n",
      "Epoch: 13, Accuracy on validation set: 0.4334109676053356\n",
      "Epoch: 14, Accuracy on validation set: 0.3857717552403134\n",
      "Epoch: 15, Accuracy on validation set: 0.42197755663773023\n",
      "Epoch: 16, Accuracy on validation set: 0.39974592420071986\n",
      "Epoch: 17, Accuracy on validation set: 0.4395511327546051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Accuracy on validation set: 0.4838026677958924\n",
      "Epoch: 19, Accuracy on validation set: 0.4088503070082575\n",
      "Epoch: 20, Accuracy on validation set: 0.482108829134025\n",
      "Epoch: 21, Accuracy on validation set: 0.4946008892652975\n",
      "Epoch: 22, Accuracy on validation set: 0.4361634554308702\n",
      "Epoch: 23, Accuracy on validation set: 0.44060978191827227\n",
      "Epoch: 24, Accuracy on validation set: 0.4920601312724963\n",
      "Epoch: 25, Accuracy on validation set: 0.45754816853694685\n",
      "Epoch: 26, Accuracy on validation set: 0.4647469828498835\n",
      "Epoch: 27, Accuracy on validation set: 0.42134236713952994\n",
      "Epoch: 28, Accuracy on validation set: 0.48126190980309125\n",
      "Epoch: 29, Accuracy on validation set: 0.40546262968452257\n",
      "Epoch: 30, Accuracy on validation set: 0.45543087020961254\n",
      "Epoch: 31, Accuracy on validation set: 0.4435739995765403\n",
      "Early stop\n",
      "Time: 2023-04-16T11:06:07.076786 Experiment: trainer_oneLSTM_params_widedouble\n",
      "Running experiment for  experiments_rnn/trainer_oneLSTM_params_widedouble_run_10\n",
      "Epoch: 1, Accuracy on validation set: 0.16216216216216217\n",
      "Epoch: 2, Accuracy on validation set: 0.18876356671632263\n",
      "Epoch: 3, Accuracy on validation set: 0.24324324324324326\n",
      "Epoch: 4, Accuracy on validation set: 0.23175143647584592\n",
      "Epoch: 5, Accuracy on validation set: 0.26537561183230474\n",
      "Epoch: 6, Accuracy on validation set: 0.2451585443711428\n",
      "Epoch: 7, Accuracy on validation set: 0.25367099382847413\n",
      "Epoch: 8, Accuracy on validation set: 0.27984677590976803\n",
      "Epoch: 9, Accuracy on validation set: 0.3138965737390934\n",
      "Epoch: 10, Accuracy on validation set: 0.3453926367312194\n",
      "Epoch: 11, Accuracy on validation set: 0.3390082996382209\n",
      "Epoch: 12, Accuracy on validation set: 0.3745477761225793\n",
      "Epoch: 13, Accuracy on validation set: 0.4241327942115344\n",
      "Epoch: 14, Accuracy on validation set: 0.40221323685890614\n",
      "Epoch: 15, Accuracy on validation set: 0.268567780378804\n",
      "Epoch: 16, Accuracy on validation set: 0.3607150457544158\n",
      "Epoch: 17, Accuracy on validation set: 0.3519897850606512\n",
      "Epoch: 18, Accuracy on validation set: 0.32900617152585654\n",
      "Epoch: 19, Accuracy on validation set: 0.3624175356458821\n",
      "Epoch: 20, Accuracy on validation set: 0.37135560757608\n",
      "Epoch: 21, Accuracy on validation set: 0.34496701425835286\n",
      "Epoch: 22, Accuracy on validation set: 0.41774845711853587\n",
      "Epoch: 23, Accuracy on validation set: 0.37667588848691214\n",
      "Early stop\n"
     ]
    }
   ],
   "source": [
    "for experiment in experiment_list:\n",
    "    print(\n",
    "        \"Time:\", datetime.datetime.now().isoformat(),\n",
    "        \"Experiment:\", experiment[0]\n",
    "    )\n",
    "    try:\n",
    "        run_experiment(*experiment)\n",
    "    except Exception as e:\n",
    "        print(\"Error occured, skipping...\\n\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a091f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
