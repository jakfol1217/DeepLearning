{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7052fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "from DeepLearning.Project1.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b341806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File based on https://github.com/huyvnphan/PyTorch_CIFAR10/\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, num_classes=10, avgpool_size=(1,1)):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(avgpool_size)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * avgpool_size[0] * avgpool_size[1], 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    " # Create convolution part of VGG11_bn archhitecture\n",
    "def make_vgg11_bn_layers(cfg = None):\n",
    "    if cfg == None:\n",
    "        cfg = [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"]\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == \"M\":\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    " # Create VGG11_bn model\n",
    "def vgg11_bn(device=\"cpu\", num_classes=10):\n",
    "    model = VGG(make_vgg11_bn_layers(), num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "vgg_cfg = {\n",
    "    '8':  [64, 'M', 128, 'M', 256, 'M', 512, 'M', 512, 'M'],\n",
    "    '11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    '13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    '16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    '19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
    "          512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def eval_accuracy(model, dataloader, training_device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model.to(training_device)\n",
    "        correct = 0\n",
    "        all_so_far = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(training_device), labels.to(training_device)\n",
    "            pred = torch.argmax(model(inputs), dim=1)\n",
    "\n",
    "            all_so_far += labels.size().numel()\n",
    "            correct += torch.sum(pred.eq(labels)).item()\n",
    "    model.train()\n",
    "    return correct/all_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc0dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_to_ram(model):\n",
    "    from copy import deepcopy\n",
    "    return deepcopy(model).cpu()\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience = 3, backup_method=backup_to_ram):\n",
    "        self.patience = patience\n",
    "        self.current = 0\n",
    "        \n",
    "        self.backup_method = backup_method\n",
    "        \n",
    "        self.best_backup = None\n",
    "        self.best_accuracy = 0.\n",
    "\n",
    "    def should_continue(self, accuracy, model = None):\n",
    "        if self.best_accuracy < accuracy:\n",
    "            self.current = 0\n",
    "            self.best_accuracy = accuracy\n",
    "            if model is not None:\n",
    "                self.best_backup = self.backup_method(model)\n",
    "            return True\n",
    "        \n",
    "        self.current += 1\n",
    "        \n",
    "        if self.current >= self.patience:\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d055caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, criterion, dataloader_train, training_device):\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader_train:\n",
    "        inputs, labels = inputs.to(training_device), labels.to(training_device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(inputs), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def train_one_run(model, optimizer, criterion,\n",
    "                  dataloader_train, dataloader_val,\n",
    "                  max_epochs, early_stopper, \n",
    "                  trajectory, \n",
    "                  training_device='cuda', mem_bg_allocated=0, mem_bg_reserved=0,\n",
    "                  *_args, **_kwargs):\n",
    "    model.train()\n",
    "    model.to(training_device)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_one_epoch(model, optimizer, criterion, dataloader_train, training_device)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        training_accuracy = eval_accuracy(model, dataloader_train, training_device)\n",
    "        validation_accuracy = eval_accuracy(model, dataloader_val, training_device)\n",
    "        print(\"Epoch: {}, Accuracy on validation set: {}\".format(epoch, validation_accuracy))\n",
    "        \n",
    "        trajectory.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train\": training_accuracy,\n",
    "            \"validation\": validation_accuracy,\n",
    "            \"start_time\": start_time,\n",
    "            \"duration\": end_time - start_time,\n",
    "            \"memory_allocated_mb\": (torch.cuda.memory_allocated() - mem_bg_allocated)/1024/1024,\n",
    "            \"memory_reserved_mb\": (torch.cuda.memory_reserved() - mem_bg_reserved)/1024/1024,\n",
    "        })\n",
    "        \n",
    "        \n",
    "        if not early_stopper.should_continue(validation_accuracy, model):\n",
    "            print(\"Early stop\")\n",
    "            return early_stopper.best_backup\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02660cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_experiment(arch_name, model_factory, aug_name, aug_factory, train_name, train_func, run):\n",
    "    path = f\"experiments/train_{train_name}_aug_{aug_name}_arch_{arch_name}_{run}_\"\n",
    "    \n",
    "    # \n",
    "    import os\n",
    "    try:\n",
    "        if os.stat(path + \"report.json\").st_size != 0:\n",
    "            print(\"Report exists already for \" + path[:-1] + \". Skipping...\")\n",
    "            return\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    model, trajectory, validation_accuracy = train_func(aug_factory, model_factory)\n",
    "    \n",
    "    with open(path + \"report.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"name\": arch_name,\n",
    "                \"run\": run,\n",
    "                \"augment\": aug_name,\n",
    "                \"train\": train_name, \n",
    "                \"best_accuracy_validation\": validation_accuracy,\n",
    "                \"time_generated\": datetime.datetime.now().isoformat(),\n",
    "                \"trajectory\": trajectory\n",
    "            },\n",
    "            f\n",
    "        )\n",
    "    torch.save(model, path + \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704fbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasterlearn(aug_factory, model_factory):\n",
    "    torch.cuda.empty_cache()\n",
    "    mem_bg_allocated = torch.cuda.memory_allocated()\n",
    "    mem_bg_reserved = torch.cuda.memory_reserved()\n",
    "    \n",
    "    train, test, val = aug_factory()\n",
    "    model = model_factory()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    early_stopper = EarlyStopper(patience = 5)\n",
    "    trajectory = []\n",
    "    model = train_one_run(model, optimizer, criterion,\n",
    "                          train, val,\n",
    "                          200, early_stopper,\n",
    "                          trajectory, \n",
    "                          mem_bg_allocated=mem_bg_allocated,\n",
    "                          mem_bg_reserved=mem_bg_reserved)\n",
    "    validation_accuracy = eval_accuracy(model, val, \"cuda\")\n",
    "    return model, trajectory, validation_accuracy\n",
    "\n",
    "def morepatient(aug_factory, model_factory):\n",
    "    torch.cuda.empty_cache()\n",
    "    mem_bg_allocated = torch.cuda.memory_allocated()\n",
    "    mem_bg_reserved = torch.cuda.memory_reserved()\n",
    "    \n",
    "    train, test, val = aug_factory()\n",
    "    model = model_factory()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    early_stopper = EarlyStopper(patience = 10)\n",
    "    trajectory = []\n",
    "    model = train_one_run(model, optimizer, criterion,\n",
    "                          train, val,\n",
    "                          200, early_stopper,\n",
    "                          trajectory, \n",
    "                          mem_bg_allocated=mem_bg_allocated,\n",
    "                          mem_bg_reserved=mem_bg_reserved)\n",
    "    validation_accuracy = eval_accuracy(model, val, \"cuda\")    \n",
    "    return model, trajectory, validation_accuracy\n",
    "\n",
    "def smallbatch(aug_factory, model_factory):\n",
    "    torch.cuda.empty_cache()\n",
    "    mem_bg_allocated = torch.cuda.memory_allocated()\n",
    "    mem_bg_reserved = torch.cuda.memory_reserved()\n",
    "    \n",
    "    train, test, val = aug_factory(bs=16)\n",
    "    model = model_factory()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    early_stopper = EarlyStopper(patience = 5)\n",
    "    trajectory = []\n",
    "    model = train_one_run(model, optimizer, criterion,\n",
    "                          train, val,\n",
    "                          200, early_stopper,\n",
    "                          trajectory, \n",
    "                          mem_bg_allocated=mem_bg_allocated,\n",
    "                          mem_bg_reserved=mem_bg_reserved)\n",
    "    validation_accuracy = eval_accuracy(model, val, \"cuda\")    \n",
    "    return model, trajectory, validation_accuracy\n",
    "\n",
    "\n",
    "def bigbatch(aug_factory, model_factory):\n",
    "    torch.cuda.empty_cache()\n",
    "    mem_bg_allocated = torch.cuda.memory_allocated()\n",
    "    mem_bg_reserved = torch.cuda.memory_reserved()\n",
    "    \n",
    "    train, test, val = aug_factory(bs=1024)\n",
    "    model = model_factory()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    early_stopper = EarlyStopper(patience = 5)\n",
    "    trajectory = []\n",
    "    model = train_one_run(model, optimizer, criterion,\n",
    "                          train, val,\n",
    "                          200, early_stopper,\n",
    "                          trajectory, \n",
    "                          mem_bg_allocated=mem_bg_allocated,\n",
    "                          mem_bg_reserved=mem_bg_reserved)\n",
    "    validation_accuracy = eval_accuracy(model, val, \"cuda\")    \n",
    "    return model, trajectory, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6ec4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(bs=128):\n",
    "    return augmented_cifar10_dataset_randomflip(bs=bs)\n",
    "\n",
    "def smallrotate(bs=128):\n",
    "    return augmented_cifar10_dataset_rotate_randomapply(5, bs=bs)\n",
    "\n",
    "def fliprotate(bs=128):\n",
    "    return augmented_cifar10_dataset_randomflip_rotate_randomapply(5, bs=bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a0094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_from_cfg(cfg_key):\n",
    "    return VGG(\n",
    "        make_vgg11_bn_layers(cfg=vgg_cfg[cfg_key]),\n",
    "        num_classes=10,\n",
    "        avgpool_size=(1,1)\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "def vgg13():\n",
    "    return vgg_from_cfg(\"13\")\n",
    "\n",
    "def vgg16():\n",
    "    return vgg_from_cfg(\"16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2bc1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = [\n",
    "    (\n",
    "        archfactory.__name__, \n",
    "        archfactory, \n",
    "        augfactory.__name__, \n",
    "        augfactory, \n",
    "        trainfunc.__name__,\n",
    "        trainfunc,\n",
    "        str(run)\n",
    "    )\n",
    "    for run in range(1, 10) \n",
    "    for archfactory in [vgg13, vgg16]\n",
    "    for augfactory in [flip, smallrotate,  fliprotate]\n",
    "    for trainfunc in [fasterlearn, morepatient]\n",
    "] + [\n",
    "    (\n",
    "        archfactory.__name__, \n",
    "        archfactory, \n",
    "        \"none\", # memory is the only interesting part for batch size\n",
    "        load_cifar10_dataloaders_validation, \n",
    "        trainfunc.__name__,\n",
    "        trainfunc,\n",
    "        str(1) # memory is the only interesting part for batch size\n",
    "    )\n",
    "    for archfactory in [vgg13, vgg16]\n",
    "    for trainfunc in [smallbatch, bigbatch]    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfb5e671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c45ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2023-03-20T14:31:52.355162 vgg13 flip fasterlearn 1\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "for experiment in experiment_list:\n",
    "    print(\n",
    "        \"Time:\", datetime.datetime.now().isoformat(),\n",
    "        *experiment[::2]\n",
    "    )\n",
    "    try:\n",
    "        run_train_experiment(*experiment)\n",
    "    except Exception as e:\n",
    "        print(\"Error occured, skipping...\\n\", repr(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
